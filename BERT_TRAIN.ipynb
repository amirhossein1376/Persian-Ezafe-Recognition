{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"BERT.ipynb","provenance":[],"collapsed_sections":["baWMO1ugzvJG","L87tbNADz2J7"]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"baWMO1ugzvJG"},"source":["# **Preparing**"]},{"cell_type":"markdown","metadata":{"id":"OXqZccmGdXmu"},"source":["**Import Necessary Libraries**"]},{"cell_type":"code","metadata":{"id":"m-PMG3m9dUI7"},"source":["from google.colab import drive\n","import pandas as pd\n","import json\n","import numpy as np\n","import math\n","from sklearn.model_selection import train_test_split\n","from sklearn.utils import shuffle\n","from tensorflow.keras.utils import to_categorical\n","import string\n","import matplotlib.pyplot as plt\n","from collections import Counter\n","import os\n","import sys\n","import logging\n","from pathlib import Path\n","import random\n","import gensim\n","import  codecs\n","from tqdm import tqdm\n","from fastprogress.fastprogress import master_bar, progress_bar\n","plt.style.use(\"ggplot\")"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"LH7xp0p7ddqq"},"source":["**Configurations To Use In The Script**"]},{"cell_type":"code","metadata":{"id":"7327Kj5VjZ1u"},"source":["normalizer = {\n"," \"؛\" : \";\",\n"," \"«\" : \"<<\",\n"," \"؟\" : \"?\",\n"," \"²\" : \".\",\n"," \"،\" : \",\",\n"," \"»\" : \">>\",\n"," \"×\" : \"*\",\n"," \"ة\" : \"ه\",\n"," \"–\" : \"-\",\n"," \"ؤ\" : \"و\",\n"," \"½\" : \".\",\n"," \"ئ\" : \"ی\",\n"," \"…\" : \".\",\n","}"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"E7etEGdvdHhQ"},"source":["config = {\n","    \"parent_dir\" : \"/content/gdrive/MyDrive/NLP-Spring 99-00/HW/HW2/\",\n","    \"model_dir\" : \"/content/gdrive/MyDrive/NLP-Spring 99-00/HW/HW2/Ph1/Model\",\n","    \"train_data_name\" : \"train.data\",\n","    \"length_threshold\" : 128\n","}"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Ycol6qwXdiLH"},"source":["**Mount Drive**"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ot3pCIxtdcXn","executionInfo":{"status":"ok","timestamp":1622482373365,"user_tz":-270,"elapsed":21,"user":{"displayName":"Seyed Hesam Alavian","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gjgsc-HNxByrYuG3FKJ5e5Crm30oJbwEqMgy1EY=s64","userId":"03839551047150628183"}},"outputId":"1d392e6b-3922-4940-f649-8fe6b633f385"},"source":["drive.mount(\"/content/gdrive\")"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"uzQFPVl1QuQV"},"source":["**Read Data**"]},{"cell_type":"code","metadata":{"id":"wQprGId5dlPR"},"source":["#read raw data from path that defined in the config dictionary\n","with open(config[\"parent_dir\"] + config[\"train_data_name\"] , 'r' , encoding=\"utf-8\") as f:\n","    raw_data = f.readlines()\n","\n","#apply some correctionscorrections on the raw data and convert some characters to correct form\n","normalized_data = []\n","for line in raw_data:\n","    normalized_line = line\n","    for item in normalizer:\n","        normalized_line = normalized_line.replace(item , normalizer[item])\n","    normalized_data.append(normalized_line)\n","del raw_data"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"L87tbNADz2J7"},"source":["# **Preprocess**"]},{"cell_type":"markdown","metadata":{"id":"hC8VuXCHHFwR"},"source":["**Break Text Into Sentences And Keep Labels**"]},{"cell_type":"code","metadata":{"id":"yS1hceoxedTO"},"source":["split_labeled_text = []\n","sentence = []\n","for line in normalized_data:\n","    if len(line)==0 or line[0]==\"\\n\":\n","        if len(sentence) > 0:\n","            split_labeled_text.append(sentence)\n","            sentence = []\n","        continue\n","    splits = line.split(' ')\n","    sentence.append([splits[0],splits[-1].rstrip(\"\\n\")])\n","    \n","if len(sentence) > 0:\n","    split_labeled_text.append(sentence)\n","    sentence = []\n","\n","#split_labeled_text = [[[word , label] , [word , label] , [word , label] , ... , [word , label]] , ....]"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"PBVLhYGGHRJJ"},"source":["**Seperate Each Sentence Into Pairs Of (Word , Label)**"]},{"cell_type":"code","metadata":{"id":"tiGtiU0jcOh7"},"source":["sentences = []\n","labels = []\n","for data in split_labeled_text:\n","    sentence = []\n","    label = []\n","    for item in data:\n","        sentence.append(item[0])\n","        label.append(item[1])\n","    sentences.append(sentence)\n","    labels.append(label)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"MHtTcA-yILNx"},"source":["**Extract All Words In The Sentences**"]},{"cell_type":"code","metadata":{"id":"PAq_bGRQb_Nw","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1622482384450,"user_tz":-270,"elapsed":1461,"user":{"displayName":"Seyed Hesam Alavian","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gjgsc-HNxByrYuG3FKJ5e5Crm30oJbwEqMgy1EY=s64","userId":"03839551047150628183"}},"outputId":"a4f75d5d-92d4-46ff-af5c-318ad1fddd2c"},"source":["word_list = set()\n","char_list = set()\n","for sent in sentences:\n","    for token in sent:\n","        word_list.add(token)\n","\n","word_list = list(word_list)\n","len(word_list)"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["57296"]},"metadata":{"tags":[]},"execution_count":8}]},{"cell_type":"markdown","metadata":{"id":"tdosryapIfWK"},"source":["**Create Dictionary To Save Each Label With Corresponding Id**"]},{"cell_type":"code","metadata":{"id":"PvGzGLmFb_Bd"},"source":["label_2_idx = {\"gen_negative\" : 0 , \"gen_positive\" : 1}\n","idx_2_label = {i: l for l, i in label_2_idx.items()}"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"fcet_BNPR74U"},"source":["labels_to_idx = []\n","for data in labels:\n","    labels_sentence = []\n","    for item in data:\n","        labels_sentence.append(label_2_idx[item])\n","    labels_to_idx.append(labels_sentence)\n","labels = labels_to_idx"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"erIhRyFNI1Ne"},"source":["**Shuffle Sentences And Corresponding Labels To Reduce Dependencies Between Successive Sentences**"]},{"cell_type":"code","metadata":{"id":"hB_PEAhAb6IE"},"source":["sentences, labels = shuffle(sentences, labels, random_state=42)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Mikfwa9bJKHr"},"source":["**Split Data To Train, Test And Validation Sets**"]},{"cell_type":"code","metadata":{"id":"ZbrwTbTsb_Qb"},"source":["train_sentences, valid_sentences, train_labels, valid_labels = train_test_split(sentences, labels, test_size=0.2)\n","valid_sentences, test_sentences, valid_labels, test_labels = train_test_split(valid_sentences, valid_labels, test_size=0.5)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"nA_0OZmFz7KK"},"source":["# **Model**"]},{"cell_type":"code","metadata":{"id":"Fpot1wE60tSB","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1622482386955,"user_tz":-270,"elapsed":2521,"user":{"displayName":"Seyed Hesam Alavian","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gjgsc-HNxByrYuG3FKJ5e5Crm30oJbwEqMgy1EY=s64","userId":"03839551047150628183"}},"outputId":"6cd3bef3-fc23-4ecc-a1cd-73af673dc27f"},"source":["!pip install transformers"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Requirement already satisfied: transformers in /usr/local/lib/python3.7/dist-packages (4.6.1)\n","Requirement already satisfied: sacremoses in /usr/local/lib/python3.7/dist-packages (from transformers) (0.0.45)\n","Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.41.1)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.0.12)\n","Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from transformers) (4.0.1)\n","Requirement already satisfied: huggingface-hub==0.0.8 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.0.8)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.19.5)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n","Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n","Requirement already satisfied: tokenizers<0.11,>=0.10.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.10.3)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from transformers) (20.9)\n","Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.15.0)\n","Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (7.1.2)\n","Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.0.1)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers) (3.4.1)\n","Requirement already satisfied: typing-extensions>=3.6.4; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers) (3.7.4.3)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2020.12.5)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n","Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->transformers) (2.4.7)\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"wgg-oZ4J0Ltj"},"source":["**Impot Libraries**"]},{"cell_type":"code","metadata":{"id":"Jc1gibyJ-g9D"},"source":["import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","import pandas as pd\n","import transformers\n","import tensorflow as tf\n","from torch.autograd import Variable\n","from torch.utils.data import Dataset, DataLoader, RandomSampler, SequentialSampler\n","from transformers import BertForTokenClassification, BertTokenizer, BertConfig, BertModel"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"NvmlYDoY0QNb"},"source":["**Define Some Important Variables**"]},{"cell_type":"code","metadata":{"id":"OfN__XNS-ii9"},"source":["MAX_LEN = 128\n","TRAIN_BATCH_SIZE = 32\n","VALID_BATCH_SIZE = 32\n","LEARNING_RATE = 2e-05\n","tokenizer = BertTokenizer.from_pretrained('HooshvareLab/bert-base-parsbert-uncased')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"UxcgqB3Q0UyG"},"source":["**Custom Class For Feeding Data Into The Pars Bert Model**"]},{"cell_type":"code","metadata":{"id":"MaeM-j4t_FMU"},"source":["class CustomDataset(Dataset):\n","    def __init__(self, tokenizer, sentences, labels, max_len):\n","        self.len = len(sentences)\n","        self.sentences = sentences\n","        self.labels = labels\n","        self.tokenizer = tokenizer\n","        self.max_len = max_len\n","        \n","    def __getitem__(self, index):\n","        sentence = self.sentences[index]\n","        inputs = self.tokenizer.encode_plus(\n","            sentence,\n","            None,\n","            truncation=True,\n","            is_split_into_words=True,\n","            add_special_tokens=True,\n","            max_length=self.max_len,\n","            pad_to_max_length=True,\n","            return_token_type_ids=True\n","        )\n","        ids = inputs['input_ids']\n","        mask = inputs['attention_mask']\n","        try:\n","            label = self.labels[index]\n","            label.extend([-100]*self.max_len)\n","            label=label[:self.max_len]\n","        except:\n","            print(index)\n","            print(sentence)\n","            raise\n","\n","        return {\n","            'input_ids': torch.tensor(ids, dtype=torch.long),\n","            'attention_mask': torch.tensor(mask, dtype=torch.long),\n","            'labels': label\n","        } \n","    \n","    def __len__(self):\n","        return self.len"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"3sHyMyw-0e7Y"},"source":["**Create Train, Test And Validation Sets**"]},{"cell_type":"code","metadata":{"id":"bfYVBX2-_g3P"},"source":["training_set = CustomDataset(tokenizer, train_sentences, train_labels , MAX_LEN)\n","validation_set = CustomDataset(tokenizer, valid_sentences, valid_labels , MAX_LEN)\n","testing_set = CustomDataset(tokenizer, test_sentences, test_labels , MAX_LEN)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"9EtsyKde6x2B","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1622482393150,"user_tz":-270,"elapsed":4548,"user":{"displayName":"Seyed Hesam Alavian","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gjgsc-HNxByrYuG3FKJ5e5Crm30oJbwEqMgy1EY=s64","userId":"03839551047150628183"}},"outputId":"aa98a03c-4639-4abf-affe-eceb6e09b4af"},"source":["!pip install datasets\n","!pip install seqeval"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Requirement already satisfied: datasets in /usr/local/lib/python3.7/dist-packages (1.7.0)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from datasets) (1.19.5)\n","Requirement already satisfied: dill in /usr/local/lib/python3.7/dist-packages (from datasets) (0.3.3)\n","Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from datasets) (4.0.1)\n","Requirement already satisfied: multiprocess in /usr/local/lib/python3.7/dist-packages (from datasets) (0.70.11.1)\n","Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from datasets) (1.1.5)\n","Requirement already satisfied: fsspec in /usr/local/lib/python3.7/dist-packages (from datasets) (2021.5.0)\n","Requirement already satisfied: tqdm<4.50.0,>=4.27 in /usr/local/lib/python3.7/dist-packages (from datasets) (4.41.1)\n","Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.7/dist-packages (from datasets) (2.23.0)\n","Requirement already satisfied: huggingface-hub<0.1.0 in /usr/local/lib/python3.7/dist-packages (from datasets) (0.0.8)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from datasets) (20.9)\n","Requirement already satisfied: xxhash in /usr/local/lib/python3.7/dist-packages (from datasets) (2.0.2)\n","Requirement already satisfied: pyarrow<4.0.0,>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from datasets) (3.0.0)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->datasets) (3.4.1)\n","Requirement already satisfied: typing-extensions>=3.6.4; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->datasets) (3.7.4.3)\n","Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas->datasets) (2.8.1)\n","Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.7/dist-packages (from pandas->datasets) (2018.9)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets) (2.10)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets) (3.0.4)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets) (2020.12.5)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets) (1.24.3)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<0.1.0->datasets) (3.0.12)\n","Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->datasets) (2.4.7)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.7.3->pandas->datasets) (1.15.0)\n","Requirement already satisfied: seqeval in /usr/local/lib/python3.7/dist-packages (1.2.2)\n","Requirement already satisfied: scikit-learn>=0.21.3 in /usr/local/lib/python3.7/dist-packages (from seqeval) (0.22.2.post1)\n","Requirement already satisfied: numpy>=1.14.0 in /usr/local/lib/python3.7/dist-packages (from seqeval) (1.19.5)\n","Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.21.3->seqeval) (1.0.1)\n","Requirement already satisfied: scipy>=0.17.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.21.3->seqeval) (1.4.1)\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"kjKJIi5i_qCZ","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1622482396141,"user_tz":-270,"elapsed":3001,"user":{"displayName":"Seyed Hesam Alavian","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gjgsc-HNxByrYuG3FKJ5e5Crm30oJbwEqMgy1EY=s64","userId":"03839551047150628183"}},"outputId":"3aa0d6a5-5fff-4a3a-f838-aed5a702c7ec"},"source":["from transformers import AutoModelForTokenClassification, TrainingArguments, Trainer\n","\n","model = AutoModelForTokenClassification.from_pretrained('HooshvareLab/bert-base-parsbert-uncased', num_labels=2)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Some weights of the model checkpoint at HooshvareLab/bert-base-parsbert-uncased were not used when initializing BertForTokenClassification: ['cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.predictions.decoder.bias', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight']\n","- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of BertForTokenClassification were not initialized from the model checkpoint at HooshvareLab/bert-base-parsbert-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"],"name":"stderr"}]},{"cell_type":"markdown","metadata":{"id":"AV3nZycT0mzM"},"source":["**Train Arguments**"]},{"cell_type":"code","metadata":{"id":"v6WM3uc0_rfU"},"source":["args = TrainingArguments(\n","    \"test-ezafe\",\n","    evaluation_strategy = \"epoch\",\n","    learning_rate=2e-5,\n","    per_device_train_batch_size=TRAIN_BATCH_SIZE,\n","    per_device_eval_batch_size=VALID_BATCH_SIZE,\n","    num_train_epochs=2,\n","    weight_decay=0.01,\n","    save_total_limit=1,\n",")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"8qS4NFUKBzTW"},"source":["from datasets import load_metric\n","metric = load_metric(\"seqeval\")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"06M24bAnBzQI","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1622482396150,"user_tz":-270,"elapsed":41,"user":{"displayName":"Seyed Hesam Alavian","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gjgsc-HNxByrYuG3FKJ5e5Crm30oJbwEqMgy1EY=s64","userId":"03839551047150628183"}},"outputId":"95bda1c4-1134-45e8-9a4a-a14f63c1b537"},"source":["labels = list(label_2_idx.keys())\n","metric.compute(predictions=[labels], references=[labels])"],"execution_count":null,"outputs":[{"output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: gen_negative seems not to be NE tag.\n","  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n","/usr/local/lib/python3.7/dist-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: gen_positive seems not to be NE tag.\n","  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n"],"name":"stderr"},{"output_type":"execute_result","data":{"text/plain":["{'en_negative': {'f1': 1.0, 'number': 1, 'precision': 1.0, 'recall': 1.0},\n"," 'en_positive': {'f1': 1.0, 'number': 1, 'precision': 1.0, 'recall': 1.0},\n"," 'overall_accuracy': 1.0,\n"," 'overall_f1': 1.0,\n"," 'overall_precision': 1.0,\n"," 'overall_recall': 1.0}"]},"metadata":{"tags":[]},"execution_count":23}]},{"cell_type":"markdown","metadata":{"id":"4lbsIxdk0qhx"},"source":["**A Function For Computing Evaluation Metrics**"]},{"cell_type":"code","metadata":{"id":"srflJdCJBzKl"},"source":["import numpy as np\n","\n","def compute_metrics(p):\n","    predictions , labels = p\n","    predictions = np.argmax(predictions, axis=2)\n","\n","    true_predictions = [\n","        [idx_2_label[p] for (p, l) in zip(prediction, label) if l != -100]\n","        for prediction, label in zip(predictions, labels)\n","    ]\n","    true_labels = [\n","        [idx_2_label[p] for (p, l) in zip(prediction, label) if l != -100]\n","        for prediction, label in zip(predictions, labels)\n","    ]\n","\n","    results = metric.compute(predictions=true_predictions, references=true_labels)\n","    return {\n","        \"precision\": results[\"overall_precision\"],\n","        \"recall\": results[\"overall_recall\"],\n","        \"f1\": results[\"overall_f1\"],\n","        \"accuracy\": results[\"overall_accuracy\"],\n","    }"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"dMHExe1HPt_W"},"source":["from transformers import DataCollatorForTokenClassification\n","\n","data_collator = DataCollatorForTokenClassification(tokenizer)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"jIgIrsoM00nj"},"source":["**Train The Model**"]},{"cell_type":"code","metadata":{"id":"cacJLS_4_lFe"},"source":["trainer = Trainer(\n","    model,\n","    args,\n","    train_dataset=training_set,\n","    eval_dataset=validation_set,\n","    data_collator=data_collator,\n","    tokenizer=tokenizer,\n","    compute_metrics=compute_metrics\n",")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"mjN0mqLzD1-o","colab":{"base_uri":"https://localhost:8080/","height":1000},"executionInfo":{"status":"ok","timestamp":1622485436512,"user_tz":-270,"elapsed":3035357,"user":{"displayName":"Seyed Hesam Alavian","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gjgsc-HNxByrYuG3FKJ5e5Crm30oJbwEqMgy1EY=s64","userId":"03839551047150628183"}},"outputId":"214a4225-1195-4778-a11f-f81b93d464af"},"source":["trainer.train()"],"execution_count":null,"outputs":[{"output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2110: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.7/dist-packages/transformers/data/data_collator.py:184: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  sequence_length = torch.tensor(batch[\"input_ids\"]).shape[1]\n","/usr/local/lib/python3.7/dist-packages/transformers/data/data_collator.py:191: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  batch = {k: torch.tensor(v, dtype=torch.int64) for k, v in batch.items()}\n"],"name":"stderr"},{"output_type":"display_data","data":{"text/html":["\n","    <div>\n","      \n","      <progress value='3506' max='3506' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [3506/3506 50:27, Epoch 2/2]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: left;\">\n","      <th>Epoch</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","      <th>Precision</th>\n","      <th>Recall</th>\n","      <th>F1</th>\n","      <th>Accuracy</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>1</td>\n","      <td>0.082800</td>\n","      <td>0.062748</td>\n","      <td>1.000000</td>\n","      <td>1.000000</td>\n","      <td>1.000000</td>\n","      <td>1.000000</td>\n","    </tr>\n","    <tr>\n","      <td>2</td>\n","      <td>0.053800</td>\n","      <td>0.055342</td>\n","      <td>1.000000</td>\n","      <td>1.000000</td>\n","      <td>1.000000</td>\n","      <td>1.000000</td>\n","    </tr>\n","  </tbody>\n","</table><p>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2110: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.7/dist-packages/transformers/data/data_collator.py:184: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  sequence_length = torch.tensor(batch[\"input_ids\"]).shape[1]\n","/usr/local/lib/python3.7/dist-packages/transformers/data/data_collator.py:191: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  batch = {k: torch.tensor(v, dtype=torch.int64) for k, v in batch.items()}\n","/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2110: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.7/dist-packages/transformers/data/data_collator.py:184: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  sequence_length = torch.tensor(batch[\"input_ids\"]).shape[1]\n","/usr/local/lib/python3.7/dist-packages/transformers/data/data_collator.py:191: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  batch = {k: torch.tensor(v, dtype=torch.int64) for k, v in batch.items()}\n","/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2110: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.7/dist-packages/transformers/data/data_collator.py:184: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  sequence_length = torch.tensor(batch[\"input_ids\"]).shape[1]\n","/usr/local/lib/python3.7/dist-packages/transformers/data/data_collator.py:191: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  batch = {k: torch.tensor(v, dtype=torch.int64) for k, v in batch.items()}\n","/usr/local/lib/python3.7/dist-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: gen_negative seems not to be NE tag.\n","  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n","/usr/local/lib/python3.7/dist-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: gen_positive seems not to be NE tag.\n","  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n","/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2110: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.7/dist-packages/transformers/data/data_collator.py:184: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  sequence_length = torch.tensor(batch[\"input_ids\"]).shape[1]\n","/usr/local/lib/python3.7/dist-packages/transformers/data/data_collator.py:191: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  batch = {k: torch.tensor(v, dtype=torch.int64) for k, v in batch.items()}\n","/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2110: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.7/dist-packages/transformers/data/data_collator.py:184: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  sequence_length = torch.tensor(batch[\"input_ids\"]).shape[1]\n","/usr/local/lib/python3.7/dist-packages/transformers/data/data_collator.py:191: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  batch = {k: torch.tensor(v, dtype=torch.int64) for k, v in batch.items()}\n","/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2110: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.7/dist-packages/transformers/data/data_collator.py:184: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  sequence_length = torch.tensor(batch[\"input_ids\"]).shape[1]\n","/usr/local/lib/python3.7/dist-packages/transformers/data/data_collator.py:191: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  batch = {k: torch.tensor(v, dtype=torch.int64) for k, v in batch.items()}\n","/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2110: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.7/dist-packages/transformers/data/data_collator.py:184: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  sequence_length = torch.tensor(batch[\"input_ids\"]).shape[1]\n","/usr/local/lib/python3.7/dist-packages/transformers/data/data_collator.py:191: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  batch = {k: torch.tensor(v, dtype=torch.int64) for k, v in batch.items()}\n","/usr/local/lib/python3.7/dist-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: gen_negative seems not to be NE tag.\n","  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n","/usr/local/lib/python3.7/dist-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: gen_positive seems not to be NE tag.\n","  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n"],"name":"stderr"},{"output_type":"execute_result","data":{"text/plain":["TrainOutput(global_step=3506, training_loss=0.09109377295747867, metrics={'train_runtime': 3028.6235, 'train_samples_per_second': 1.158, 'total_flos': 22180537052160.0, 'epoch': 2.0, 'init_mem_cpu_alloc_delta': 2164396032, 'init_mem_gpu_alloc_delta': 649538048, 'init_mem_cpu_peaked_delta': 0, 'init_mem_gpu_peaked_delta': 0, 'train_mem_cpu_alloc_delta': 237015040, 'train_mem_gpu_alloc_delta': 2012620800, 'train_mem_cpu_peaked_delta': 94105600, 'train_mem_gpu_peaked_delta': 3586833408})"]},"metadata":{"tags":[]},"execution_count":27}]},{"cell_type":"markdown","metadata":{"id":"-hglVBvZ03gS"},"source":["**Save The Model**"]},{"cell_type":"code","metadata":{"id":"6oPgqtIH1yw-"},"source":["trainer.save_model(config['parent_dir'] + 'our-pars-bert-model')\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"T4_MvKCl08jb"},"source":["**Test The Model**"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":227},"id":"2LJ3L7h4sKl3","executionInfo":{"status":"ok","timestamp":1622486405439,"user_tz":-270,"elapsed":73329,"user":{"displayName":"Seyed Hesam Alavian","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gjgsc-HNxByrYuG3FKJ5e5Crm30oJbwEqMgy1EY=s64","userId":"03839551047150628183"}},"outputId":"f102fdf9-21dd-4ccf-e543-b7c8af5130b0"},"source":["result = trainer.predict(testing_set)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2110: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.7/dist-packages/transformers/data/data_collator.py:184: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  sequence_length = torch.tensor(batch[\"input_ids\"]).shape[1]\n","/usr/local/lib/python3.7/dist-packages/transformers/data/data_collator.py:191: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  batch = {k: torch.tensor(v, dtype=torch.int64) for k, v in batch.items()}\n"],"name":"stderr"},{"output_type":"display_data","data":{"text/html":["\n","    <div>\n","      \n","      <progress value='660' max='220' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [220/220 05:31]\n","    </div>\n","    "],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: gen_negative seems not to be NE tag.\n","  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n","/usr/local/lib/python3.7/dist-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: gen_positive seems not to be NE tag.\n","  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"id":"bfLmPU66A09S"},"source":["predict_lables = np.argmax(result.predictions,-1)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"UX7mmYgUB3Hn"},"source":["true_lables = result.label_ids"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"HfhivjtvLUD8"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"qTOAhPSS1A4I"},"source":["**Sample Test**"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"OmlZETF4yNJx","executionInfo":{"status":"ok","timestamp":1622487564423,"user_tz":-270,"elapsed":768,"user":{"displayName":"Seyed Hesam Alavian","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gjgsc-HNxByrYuG3FKJ5e5Crm30oJbwEqMgy1EY=s64","userId":"03839551047150628183"}},"outputId":"6cb2f75e-5188-4aab-8408-a9efaa06f50c"},"source":["i = 999\n","for a , b , c in zip(test_sentences[i] , predict_lables[i] , true_lables[i]):\n","    print(a , b , c)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["من 0 0\n","احکام 1 1\n","خداوند 0 0\n","را 0 0\n","به 0 0\n","گونه‌ای 0 0\n","سامان‌یافته 0 0\n","به 0 0\n","گوش 1 1\n","شما 0 0\n","رساندم 0 0\n","و 0 0\n","آنچه 0 0\n","را 0 0\n","که 0 0\n","به 0 0\n","سود 1 1\n","شماست 0 0\n","پوشیده 0 0\n","نداشتم 0 0\n","و 0 0\n","همه 0 0\n","چیز 0 0\n","را 0 0\n","از 0 0\n","آغاز 0 0\n","تا 0 0\n","پایان 0 0\n","باز 0 0\n","نمودم 0 0\n",". 0 0\n"],"name":"stdout"}]}]}